{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-21 16:36:04,568] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gamma = 1.0\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add   \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size,h_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        hidden2= slim.fully_connected(hidden,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden2,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output,1)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * 2 + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n",
      "27.31\n",
      "48.18\n",
      "113.33\n",
      "174.55\n",
      "188.88\n",
      "181.13\n",
      "164.98\n",
      "186.59\n",
      "154.86\n",
      "176.19\n",
      "198.72\n",
      "200.0\n",
      "200.0\n",
      "195.99\n",
      "196.28\n",
      "192.12\n",
      "197.22\n",
      "198.29\n",
      "198.61\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "198.46\n",
      "184.4\n",
      "196.09\n",
      "198.71\n",
      "200.0\n",
      "199.92\n",
      "200.0\n",
      "200.0\n",
      "197.21\n",
      "199.99\n",
      "198.63\n",
      "198.35\n",
      "183.26\n",
      "198.17\n",
      "198.03\n",
      "198.97\n",
      "199.68\n",
      "196.61\n",
      "196.3\n",
      "197.85\n",
      "199.4\n",
      "199.14\n",
      "200.0\n",
      "199.88\n",
      "199.68\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
    "\n",
    "total_episodes = 5000 #Set total number of episodes to train agent on.\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "        \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            s1,r,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
    "            ep_history.append([s,a,r])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            if d == True:\n",
    "                #Update the network.\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict= dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "\n",
    "        \n",
    "            #Update our running tally of scores.\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.76735919e-02  -4.27363388e-02   6.79212013e-02   3.03759914e-01]\n",
      " [ -4.85283187e-02   1.51355209e-01   7.39963996e-02   3.32473673e-02]\n",
      " [ -4.55012145e-02   3.45342315e-01   7.46613469e-02  -2.35201933e-01]\n",
      " [ -3.85943682e-02   5.39322541e-01   6.99573083e-02  -5.03431620e-01]\n",
      " [ -2.78079174e-02   3.43287938e-01   5.98886759e-02  -1.89548295e-01]\n",
      " [ -2.09421586e-02   1.47362603e-01   5.60977100e-02   1.21409576e-01]\n",
      " [ -1.79949066e-02   3.41637836e-01   5.85259015e-02  -1.53060785e-01]\n",
      " [ -1.11621499e-02   5.35875066e-01   5.54646858e-02  -4.26720992e-01]\n",
      " [ -4.44648534e-04   3.40013192e-01   4.69302659e-02  -1.17081708e-01]\n",
      " [  6.35561530e-03   1.44251332e-01   4.45886318e-02   1.90030209e-01]]\n",
      "[array([-0.04767359, -0.04273634,  0.0679212 ,  0.30375991])\n",
      " array([-0.04852832,  0.15135521,  0.0739964 ,  0.03324737])\n",
      " array([-0.04550121,  0.34534231,  0.07466135, -0.23520193])\n",
      " array([-0.03859437,  0.53932254,  0.06995731, -0.50343162])\n",
      " array([-0.02780792,  0.34328794,  0.05988868, -0.1895483 ])\n",
      " array([-0.02094216,  0.1473626 ,  0.05609771,  0.12140958])\n",
      " array([-0.01799491,  0.34163784,  0.0585259 , -0.15306079])\n",
      " array([-0.01116215,  0.53587507,  0.05546469, -0.42672099])\n",
      " array([-0.00044465,  0.34001319,  0.04693027, -0.11708171])\n",
      " array([ 0.00635562,  0.14425133,  0.04458863,  0.19003021])]\n"
     ]
    }
   ],
   "source": [
    "print(np.vstack(ep_history[20:30:,0]))\n",
    "print((ep_history[20:30:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "[2017-06-20 14:40:14,816] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
    "max_ep=999\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    s = env.reset()\n",
    "    for j in range(max_ep):\n",
    "        env.render()\n",
    "        a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "        a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "        a = np.argmax(a_dist == a)\n",
    "        s1=env.step(a)\n",
    "        s = s1[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fully_connected_17/Softmax:0' shape=(1, 2) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.choice([2,4],p=[.5,.5])\n",
    "# indexes = tf.range(0, tf.shape(output)[0]) * tf.shape(output)[1] + action_holder\n",
    "#         tf.Print(indexes, [indexes], message=\"This is a: \")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.5145102"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess2 = tf.InteractiveSession()\n",
    "tf.global_variables_initializer()\n",
    "s_size=4\n",
    "h_size=160\n",
    "a_size=2\n",
    "#state_in=  tf.constant([[3.0,4.0,2.0,1.0],[3.0,4.0,2.0,1.0]])\n",
    "output=tf.constant([[1.0,2.0],[3.0,4.0],[5.0,6.0],[7.0,8.0]])\n",
    "var = tf.Variable(tf.random_normal([784, 200], stddev=0.35),\n",
    "                      name=\"weights\") \n",
    "action_holder=  tf.constant([0,0,0,1])\n",
    "reward_holder=  tf.constant([4.5,3.6,2.5,1])\n",
    "indexes = tf.range(0, 4) * 2 + action_holder\n",
    "indexes=tf.Print(indexes, [indexes], message=\"Indexes: \")\n",
    "responsible_outputs = tf.gather(tf.reshape(output, [-1]), indexes)\n",
    "responsible_outputs=tf.Print(responsible_outputs, [responsible_outputs], message=\"Responsible Outputs: \")\n",
    "loss=-tf.reduce_mean(tf.log(responsible_outputs)*reward_holder)\n",
    "gradients = tf.gradients(loss,[var])[0]\n",
    "print(gradients)\n",
    "sess2.run(loss)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
